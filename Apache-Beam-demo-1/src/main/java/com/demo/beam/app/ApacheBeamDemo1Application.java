package com.demo.beam.app;

import java.util.Arrays;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.Count;
import org.apache.beam.sdk.transforms.Filter;
import org.apache.beam.sdk.transforms.FlatMapElements;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.TypeDescriptors;
//import org.springframework.boot.SpringApplication;
//import org.springframework.boot.autoconfigure.SpringBootApplication;

//@SpringBootApplication
public class ApacheBeamDemo1Application {

	public static void main(String[] args) {
		//SpringApplication.run(ApacheBeamDemo1Application.class, args);
		String schemaRoot = System.getProperty("user.dir") + "/src/main/resources/beam/";
		// Create a PipelineOptions object. This object lets us set various execution
		// options for our pipeline, such as the runner you wish to use. This example
		// will run with the DirectRunner by default, based on the class path configured
		// in its dependencies.
		
		PipelineOptions options = PipelineOptionsFactory.create();
		
	    // In order to run your pipeline, you need to make following runner specific changes:
	    //
	    // CHANGE 1/3: Select a Beam runner, such as BlockingDataflowRunner
	    // or FlinkRunner.
	    // CHANGE 2/3: Specify runner-required options.
	    // For BlockingDataflowRunner, set project and temp location as follows:
	    // DataflowPipelineOptions dataflowOptions = options.as(DataflowPipelineOptions.class);
	    // dataflowOptions.setRunner(BlockingDataflowRunner.class);
	    // dataflowOptions.setProject("SET_YOUR_PROJECT_ID_HERE");
	    // dataflowOptions.setTempLocation("gs://SET_YOUR_BUCKET_NAME_HERE/AND_TEMP_DIRECTORY");
	    // For FlinkRunner, set the runner as follows. See {@code FlinkPipelineOptions}
	    // for more details.
	    //   options.as(FlinkPipelineOptions.class)
	    //      .setRunner(FlinkRunner.class);
		
		
		// The next step is to create a Pipeline object with the options we’ve just constructed. 
		// The Pipeline object builds up the graph of transformations to be executed, associated 
		// with that particular pipeline.
		Pipeline p = Pipeline.create(options);
		
		// The MinimalWordCount pipeline contains five transforms:
		
		// 1-. A text file Read transform is applied to the Pipeline object itself, and produces a PCollection 
		// as output. Each element in the output PCollection represents one line of text from the input file. 
		// This example uses input data stored in a publicly accessible Google Cloud Storage bucket (“gs://”).
		
		p.apply(TextIO.read().from("gs://apache-beam-samples/shakespeare/kinglear.txt"))
		
		// 2-. This transform splits the lines in PCollection<String>, where each element is an individual word in 
		// Shakespeare’s collected texts. As an alternative, it would have been possible to use a ParDo transform 
		// that invokes a DoFn (defined in-line as an anonymous class) on each element that tokenizes the text lines 
		// into individual words. The input for this transform is the PCollection of text lines generated by the 
		// previous TextIO.Read transform. The ParDo transform outputs a new PCollection, where each element represents 
		// an individual word in the text.
		
		.apply("ExtractWords",
	            FlatMapElements.into(TypeDescriptors.strings())
	            // These are Unicode property shortcuts (\p{L} for Unicode letters, \p{N} for Unicode digits). They are supported 
				// by .NET, Perl, Java, PCRE, XML, XPath, JGSoft, Ruby (1.9 and higher) and PHP
				// \p{L} matches a single code point in the category "letter".
				// \p{N} matches any kind of numeric character in any script.
                .via((String word) -> Arrays.asList(word.split("[^\\p{L}]+"))))		
		.apply(Filter.by((String word)-> !word.isEmpty()))
	
		// 3-. The SDK-provided Count transform is a generic transform that takes a PCollection of any type, and returns a PCollection 
		// of key/value pairs. Each key represents a unique element from the input collection, and each value represents the number 
		// of times that key appeared in the input collection.
		// In this pipeline, the input for Count is the PCollection of individual words generated by the previous ParDo, and the 
		// output is a PCollection of key/value pairs where each key represents a unique word in the text and the associated value 
		// is the occurrence count for each.
		
		.apply(Count.perElement())
		
		// 4-. The next transform formats each of the key/value pairs of unique words and occurrence counts into a printable string suitable 
		// for writing to an output file.
		// The map transform is a higher-level composite transform that encapsulates a simple ParDo. For each element in the input PCollection, 
		// the map transform applies a function that produces exactly one output element.
		
		.apply(MapElements.into(TypeDescriptors.strings())
				.via( 
					(KV<String, Long> wordCount) -> 
					 wordCount.getKey() + ": " + wordCount.getValue()))
	
		// 5-. A text file write transform. This transform takes the final PCollection of formatted Strings as input and writes each element to 
		// an output text file. Each element in the input PCollection represents one line of text in the resulting output file.
		
		.apply(TextIO.write().to(schemaRoot.concat("/wordcounts.txt")));
	
		p.run().waitUntilFinish();
		
		/* EXAMPLE FROM GIT APACHE BEAM
		// Create a PipelineOptions object. This object lets us set various execution
	    // options for our pipeline, such as the runner you wish to use. This example
	    // will run with the DirectRunner by default, based on the class path configured
	    // in its dependencies.
	    PipelineOptions options = PipelineOptionsFactory.create();

	    // In order to run your pipeline, you need to make following runner specific changes:
	    //
	    // CHANGE 1/3: Select a Beam runner, such as BlockingDataflowRunner
	    // or FlinkRunner.
	    // CHANGE 2/3: Specify runner-required options.
	    // For BlockingDataflowRunner, set project and temp location as follows:
	    //   DataflowPipelineOptions dataflowOptions = options.as(DataflowPipelineOptions.class);
	    //   dataflowOptions.setRunner(BlockingDataflowRunner.class);
	    //   dataflowOptions.setProject("SET_YOUR_PROJECT_ID_HERE");
	    //   dataflowOptions.setTempLocation("gs://SET_YOUR_BUCKET_NAME_HERE/AND_TEMP_DIRECTORY");
	    // For FlinkRunner, set the runner as follows. See {@code FlinkPipelineOptions}
	    // for more details.
	    //   options.as(FlinkPipelineOptions.class)
	    //      .setRunner(FlinkRunner.class);

	    // Create the Pipeline object with the options we defined above
	    Pipeline p = Pipeline.create(options);

	    // Concept #1: Apply a root transform to the pipeline; in this case, TextIO.Read to read a set
	    // of input text files. TextIO.Read returns a PCollection where each element is one line from
	    // the input text (a set of Shakespeare's texts).

	    // This example reads a public data set consisting of the complete works of Shakespeare.
	    p.apply(TextIO.read().from("gs://apache-beam-samples/shakespeare/*"))

	        // Concept #2: Apply a FlatMapElements transform the PCollection of text lines.
	        // This transform splits the lines in PCollection<String>, where each element is an
	        // individual word in Shakespeare's collected texts.
	        .apply(
	            FlatMapElements.into(TypeDescriptors.strings())
	                .via((String word) -> Arrays.asList(word.split("[^\\p{L}]+"))))
	        // We use a Filter transform to avoid empty word
	        .apply(Filter.by((String word) -> !word.isEmpty()))
	        // Concept #3: Apply the Count transform to our PCollection of individual words. The Count
	        // transform returns a new PCollection of key/value pairs, where each key represents a
	        // unique word in the text. The associated value is the occurrence count for that word.
	        .apply(Count.perElement())
	        // Apply a MapElements transform that formats our PCollection of word counts into a
	        // printable string, suitable for writing to an output file.
	        .apply(
	            MapElements.into(TypeDescriptors.strings())
	                .via(
	                    (KV<String, Long> wordCount) ->
	                        wordCount.getKey() + ": " + wordCount.getValue()))
	        // Concept #4: Apply a write transform, TextIO.Write, at the end of the pipeline.
	        // TextIO.Write writes the contents of a PCollection (in this case, our PCollection of
	        // formatted strings) to a series of text files.
	        //
	        // By default, it will write to a set of files with names like wordcounts-00001-of-00005
	        .apply(TextIO.write().to("wordcounts"));

	    p.run().waitUntilFinish();*/
	}

}
